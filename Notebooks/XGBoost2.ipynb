{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada49130-a6b0-42d1-9dff-614faf1b5f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Profile_ID', 'Age', 'BMI', 'Physical_Activity', 'Diet_Type', 'Obesity_Risk_Score', 'MC4R_Present', 'MC4R_Variant', 'PPARG_Present', 'PPARG_Variant', 'FTO_Present', 'FTO_Variant', 'LEPR_Present', 'LEPR_Variant']\n",
      "Genetic Model Accuracy: 0.725\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.95      0.90      0.93       179\n",
      "         Low       0.42      0.43      0.43        83\n",
      "      Medium       0.64      0.67      0.65       138\n",
      "\n",
      "    accuracy                           0.72       400\n",
      "   macro avg       0.67      0.67      0.67       400\n",
      "weighted avg       0.73      0.72      0.73       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[161  10   8]\n",
      " [  2  36  45]\n",
      " [  6  39  93]]\n",
      "Metrics table saved to C:\\Users\\trejan\\Desktop\\GNN\\XGB\\metrics_table.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 556, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1038, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1550, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Obesity Risk Category: High\n",
      "\n",
      "Recommended Meals:\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "2301  Sweetener, herbal extract powder from Stevia leaf          0.0   \n",
      "3281  Turkey, breast, from whole bird, enhanced, mea...        127.0   \n",
      "3985  Turkey from whole, enhanced, light meat, meat ...        127.0   \n",
      "5767  Turkey, wing, from whole bird, enhanced, meat ...        127.0   \n",
      "4302         Game meat, buffalo, water, cooked, roasted        131.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "2301       0.00   0.00   100.0  \n",
      "3281      26.97   2.08     0.0  \n",
      "3985      26.97   2.08     0.0  \n",
      "5767      26.97   2.08     0.0  \n",
      "4302      26.83   1.80     0.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = r\"C:\\Users\\trejan\\Desktop\\GNN\\XGB\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Pipeline 1: Genetic Model\n",
    "#############################\n",
    "\n",
    "# Load genetic dataset (adjust sep='\\t' if your file is tab-delimited)\n",
    "genetic_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\new_genetic_profiles.csv\"\n",
    "genetic_df = pd.read_csv(genetic_file_path)\n",
    "# Remove any extra whitespace from column names\n",
    "genetic_df.columns = genetic_df.columns.str.strip()\n",
    "print(genetic_df.columns.tolist())\n",
    "\n",
    "# Fill missing values (if any)\n",
    "genetic_df.fillna(\"None\", inplace=True)\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories using thresholds\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.3, 0.6, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Initialize a dictionary to store label encoders (for future use)\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical variables (Diet_Type, Physical_Activity)\n",
    "for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode gene variant columns as strings (so that \"None\" is encoded too)\n",
    "variant_columns = [\"MC4R_Variant\", \"PPARG_Variant\", \"FTO_Variant\", \"LEPR_Variant\"]\n",
    "for col in variant_columns:\n",
    "    genetic_df[col] = genetic_df[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target for the genetic model\n",
    "features = [\n",
    "    \"Age\", \"BMI\", \"Physical_Activity\", \"Diet_Type\",\n",
    "    \"MC4R_Present\", \"MC4R_Variant\",\n",
    "    \"PPARG_Present\", \"PPARG_Variant\",\n",
    "    \"FTO_Present\", \"FTO_Variant\",\n",
    "    \"LEPR_Present\", \"LEPR_Variant\"\n",
    "]\n",
    "target = \"Obesity_Risk_Category\"\n",
    "\n",
    "X_gen = genetic_df[features]\n",
    "y_gen = genetic_df[target]\n",
    "\n",
    "# Encode target labels (Low, Medium, High)\n",
    "target_le = LabelEncoder()\n",
    "y_encoded = target_le.fit_transform(y_gen)\n",
    "\n",
    "# Split the genetic data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gen, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train an XGBoost classifier with regularization to help prevent overfitting\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Genetic Model Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_le.classes_))\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Directory for XAI visualizations\n",
    "xai_output_dir = r\"C:\\Users\\trejan\\Desktop\\XAI\\worst XGBoost\"\n",
    "os.makedirs(xai_output_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------- 1. Feature Importance (Gain, Weight, Cover) ----------------------\n",
    "importance_types = ['gain', 'weight', 'cover']\n",
    "for imp_type in importance_types:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    xgb.plot_importance(xgb_clf, importance_type=imp_type)\n",
    "    plt.title(f'XGBoost Feature Importance ({imp_type.capitalize()})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(xai_output_dir, f'xgb_feature_importance_{imp_type}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------- 2. Permutation Importance ----------------------\n",
    "# ---------------------- 2. Permutation Importance (Fixed Plotting) ----------------------\n",
    "perm_importance = permutation_importance(xgb_clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create a DataFrame for permutation importance\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Importance Mean': perm_importance.importances_mean,\n",
    "    'Importance Std': perm_importance.importances_std\n",
    "}).sort_values(by='Importance Mean', ascending=True)\n",
    "\n",
    "# Plot using matplotlib (for error bars compatibility)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(perm_df['Feature'], perm_df['Importance Mean'], xerr=perm_df['Importance Std'], color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Importance Mean')\n",
    "plt.title('Permutation Feature Importance (with Error Bars)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(xai_output_dir, 'xgb_permutation_importance.png'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plot PDP for top features (with target=None for multi-class)\n",
    "# Specify target for a specific class (e.g., target class 0)\n",
    "target_class = 0  # or any class index you want\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "display = PartialDependenceDisplay.from_estimator(xgb_clf, X_train, features=top_pdp_features, kind='average', ax=ax, target=target_class)\n",
    "plt.suptitle(f\"Partial Dependence Plots for Class {target_class}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(os.path.join(xai_output_dir, f'xgb_pdp_class_{target_class}.png'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(output_dir, 'xgb_clf_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(xgb_clf, f)\n",
    "\n",
    "# Convert metrics to a structured table\n",
    "classification_report_dict = classification_report(y_test, y_pred, target_names=target_le.classes_, output_dict=True)\n",
    "\n",
    "# Create a DataFrame for the classification report\n",
    "report_df = pd.DataFrame(classification_report_dict).transpose()\n",
    "\n",
    "# Add accuracy and confusion matrix to the DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy'],\n",
    "    'Value': [accuracy]\n",
    "})\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, columns=target_le.classes_, index=target_le.classes_)\n",
    "conf_matrix_df['Metric'] = 'Confusion Matrix'\n",
    "\n",
    "# Combine all metrics into a single DataFrame\n",
    "metrics_table = pd.concat([metrics_df, report_df, conf_matrix_df], axis=0)\n",
    "\n",
    "# Save the metrics table as a CSV file\n",
    "metrics_table_path = os.path.join(output_dir, 'metrics_table.csv')\n",
    "metrics_table.to_csv(metrics_table_path, index=False)\n",
    "\n",
    "print(f\"Metrics table saved to {metrics_table_path}\")\n",
    "\n",
    "# Plot and save the learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(xgb_clf, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training score')\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Cross-validation score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(os.path.join(output_dir, 'learning_curve.png'))\n",
    "plt.close()\n",
    "\n",
    "# Plot and save the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_le.classes_, yticklabels=target_le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "plt.close()\n",
    "\n",
    "##################################\n",
    "# Pipeline 2: Meal Recommendation\n",
    "##################################\n",
    "\n",
    "# Load the meal dataset (assumed to be comma-delimited)\n",
    "meal_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\train.csv\"\n",
    "meal_df = pd.read_csv(meal_file_path)\n",
    "\n",
    "# Preprocess nutritional features; here we assume these columns exist\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler = StandardScaler()\n",
    "nutritional_features_scaled = scaler.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using KMeans to create diverse groups (e.g., 10 clusters)\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a meal recommendation function that uses the predicted obesity risk category\n",
    "def recommend_meals(user_profile, meal_df, model, target_le, num_meals=5):\n",
    "    \"\"\"\n",
    "    user_profile: dict with genetic feature values (unencoded, as originally provided)\n",
    "    model: trained genetic model (xgb_clf)\n",
    "    target_le: LabelEncoder for the target risk category\n",
    "    \"\"\"\n",
    "    # Convert user_profile into DataFrame\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    # Encode the categorical features in the user profile using stored encoders\n",
    "    for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col])\n",
    "    for col in variant_columns:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col].astype(str))\n",
    "    \n",
    "    # Ensure the user profile has all required features; add missing ones as 0 if needed\n",
    "    missing_cols = set(features) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0\n",
    "    user_profile_df = user_profile_df[features]\n",
    "    \n",
    "    # Predict obesity risk category using the genetic model\n",
    "    predicted_category = model.predict(user_profile_df)[0]\n",
    "    predicted_label = target_le.inverse_transform([predicted_category])[0]\n",
    "    print(f\"\\nPredicted Obesity Risk Category: {predicted_label}\")\n",
    "    \n",
    "    # Define cluster preferences based on the predicted risk\n",
    "    if predicted_label == 'Low':\n",
    "        preferred_clusters = [0, 1, 2]\n",
    "        sort_by = 'Protein_g'\n",
    "        ascending = False\n",
    "    elif predicted_label == 'Medium':\n",
    "        preferred_clusters = [3, 4, 5]\n",
    "        sort_by = 'Energy_kcal'\n",
    "        ascending = False\n",
    "    else:\n",
    "        preferred_clusters = [6, 7, 8, 9]\n",
    "        sort_by = 'Energy_kcal'\n",
    "        ascending = True\n",
    "    \n",
    "    # Filter and sort meals from the preferred clusters\n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(preferred_clusters)]\n",
    "    recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=ascending)\n",
    "    \n",
    "    print(\"\\nRecommended Meals:\")\n",
    "    print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']].head(num_meals))\n",
    "\n",
    "# Example new genetic profile for meal recommendation (using original, unencoded values)\n",
    "new_profile = {\n",
    "    \"Age\": 35,\n",
    "    \"BMI\": 28.5,\n",
    "    \"Physical_Activity\": \"Low\",      # Original string value (will be encoded)\n",
    "    \"Diet_Type\": \"High-Fat\",           # Original string value (will be encoded)\n",
    "    \"MC4R_Present\": 1,\n",
    "    \"MC4R_Variant\": \"rs17782313_TT\",    # Original string value\n",
    "    \"PPARG_Present\": 1,\n",
    "    \"PPARG_Variant\": \"rs1801282_CG\",    # Original string value\n",
    "    \"FTO_Present\": 1,\n",
    "    \"FTO_Variant\": \"rs9939609_AT\",      # Original string value\n",
    "    \"LEPR_Present\": 1,\n",
    "    \"LEPR_Variant\": \"rs1137101_AG\"       # Original string value\n",
    "}\n",
    "\n",
    "# Get meal recommendations based on the genetic model prediction\n",
    "recommend_meals(new_profile, meal_df, xgb_clf, target_le, num_meals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195e1c05-25b5-49ce-93fd-4a1a92f87372",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ---------- 1. Feature Importance (Gain, Weight, Cover) ----------\u001b[39;00m\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m---> 13\u001b[0m xgb\u001b[38;5;241m.\u001b[39mplot_importance(\u001b[43mxgb_clf\u001b[49m, importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgain\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost Feature Importance (Gain)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(xai_output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_feature_importance_gain.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_clf' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- XAI Techniques for XGBoost Model ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "import os\n",
    "\n",
    "# Create XAI output directory\n",
    "xai_output_dir = r\"C:\\Users\\trejan\\Desktop\\XAI\\worst\"\n",
    "os.makedirs(xai_output_dir, exist_ok=True)\n",
    "\n",
    "# ---------- 1. Feature Importance (Gain, Weight, Cover) ----------\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(xgb_clf, importance_type='gain', title='XGBoost Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(xai_output_dir, 'xgb_feature_importance_gain.png'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(xgb_clf, importance_type='weight', title='XGBoost Feature Importance (Weight)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(xai_output_dir, 'xgb_feature_importance_weight.png'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(xgb_clf, importance_type='cover', title='XGBoost Feature Importance (Cover)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(xai_output_dir, 'xgb_feature_importance_cover.png'))\n",
    "plt.close()\n",
    "\n",
    "# ---------- 2. Partial Dependence Plots (PDP) ----------\n",
    "# Select top N important features based on gain\n",
    "top_n = 5\n",
    "booster_scores = xgb_clf.get_booster().get_score(importance_type='gain')\n",
    "sorted_features = sorted(booster_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "important_features = [feat for feat, score in sorted_features[:top_n]]\n",
    "\n",
    "# Map feature names properly (XGBoost uses f0, f1... internally)\n",
    "feature_map = {f\"f{i}\": feat for i, feat in enumerate(features)}\n",
    "mapped_features = [int(f[1:]) for f in important_features if f.startswith('f')]\n",
    "\n",
    "# Convert X to DataFrame\n",
    "X_train_df = pd.DataFrame(X_train, columns=features)\n",
    "\n",
    "# Plot PDPs\n",
    "for feat_idx in mapped_features:\n",
    "    try:\n",
    "        display = PartialDependenceDisplay.from_estimator(\n",
    "            xgb_clf,\n",
    "            X_train_df,\n",
    "            [feat_idx],\n",
    "            feature_names=features,\n",
    "            target=0,  # For multi-class, target=0 means PDP for class 0\n",
    "            kind=\"average\"\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        display.figure_.suptitle(f\"PDP - {features[feat_idx]}\")\n",
    "        pdp_filename = f\"xgb_pdp_{features[feat_idx]}.png\"\n",
    "        display.figure_.savefig(os.path.join(xai_output_dir, pdp_filename))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped PDP for feature {features[feat_idx]} due to error: {e}\")\n",
    "\n",
    "# ---------- 3. Permutation Importance ----------\n",
    "result = permutation_importance(xgb_clf, X_train, y_train, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "\n",
    "# Create DataFrame for permutation results\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance_Mean': result.importances_mean,\n",
    "    'Importance_STD': result.importances_std\n",
    "}).sort_values(by='Importance_Mean', ascending=False)\n",
    "\n",
    "# Plot Permutation Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance_Mean', y='Feature', data=perm_importance_df, palette='viridis')\n",
    "plt.title('Permutation Importance (Accuracy Impact)')\n",
    "plt.xlabel('Mean Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(xai_output_dir, 'xgb_permutation_importance.png'))\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n✅ All XAI visualizations saved in: {xai_output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
