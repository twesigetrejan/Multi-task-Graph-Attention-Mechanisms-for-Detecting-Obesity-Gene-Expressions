{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c7745f-aab3-4f26-9202-4a8b6238f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\tf_env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5185 - loss: 1.0555 - val_accuracy: 0.7500 - val_loss: 0.6882\n",
      "Epoch 2/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6929 - loss: 0.7334 - val_accuracy: 0.7969 - val_loss: 0.5365\n",
      "Epoch 3/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7321 - loss: 0.6206 - val_accuracy: 0.8031 - val_loss: 0.4817\n",
      "Epoch 4/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7122 - loss: 0.6088 - val_accuracy: 0.8188 - val_loss: 0.4380\n",
      "Epoch 5/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7789 - loss: 0.5048 - val_accuracy: 0.8156 - val_loss: 0.4246\n",
      "Epoch 6/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7829 - loss: 0.4833 - val_accuracy: 0.8188 - val_loss: 0.4159\n",
      "Epoch 7/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8189 - loss: 0.4544 - val_accuracy: 0.8156 - val_loss: 0.4185\n",
      "Epoch 8/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8043 - loss: 0.4563 - val_accuracy: 0.8156 - val_loss: 0.4063\n",
      "Epoch 9/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8241 - loss: 0.4402 - val_accuracy: 0.8219 - val_loss: 0.3900\n",
      "Epoch 10/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8402 - loss: 0.4158 - val_accuracy: 0.8219 - val_loss: 0.3878\n",
      "Epoch 11/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8453 - loss: 0.3988 - val_accuracy: 0.8219 - val_loss: 0.3868\n",
      "Epoch 12/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8122 - loss: 0.4312 - val_accuracy: 0.8156 - val_loss: 0.3947\n",
      "Epoch 13/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8267 - loss: 0.3958 - val_accuracy: 0.8188 - val_loss: 0.3837\n",
      "Epoch 14/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8319 - loss: 0.4069 - val_accuracy: 0.8188 - val_loss: 0.3843\n",
      "Epoch 15/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8451 - loss: 0.4110 - val_accuracy: 0.8188 - val_loss: 0.3947\n",
      "Epoch 16/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8259 - loss: 0.3938 - val_accuracy: 0.8188 - val_loss: 0.3855\n",
      "Epoch 17/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8374 - loss: 0.4013 - val_accuracy: 0.8188 - val_loss: 0.3859\n",
      "Epoch 18/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8296 - loss: 0.4081 - val_accuracy: 0.8188 - val_loss: 0.3834\n",
      "Epoch 19/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8395 - loss: 0.3797 - val_accuracy: 0.8156 - val_loss: 0.3903\n",
      "Epoch 20/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8262 - loss: 0.4118 - val_accuracy: 0.8156 - val_loss: 0.3857\n",
      "Epoch 21/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8366 - loss: 0.3667 - val_accuracy: 0.8156 - val_loss: 0.3915\n",
      "Epoch 22/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8388 - loss: 0.3843 - val_accuracy: 0.8156 - val_loss: 0.3914\n",
      "Epoch 23/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8304 - loss: 0.3884 - val_accuracy: 0.8188 - val_loss: 0.3815\n",
      "Epoch 24/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8262 - loss: 0.3930 - val_accuracy: 0.8188 - val_loss: 0.3829\n",
      "Epoch 25/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8313 - loss: 0.3888 - val_accuracy: 0.8188 - val_loss: 0.3860\n",
      "Epoch 26/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8515 - loss: 0.3610 - val_accuracy: 0.8188 - val_loss: 0.3824\n",
      "Epoch 27/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8279 - loss: 0.4020 - val_accuracy: 0.8156 - val_loss: 0.3834\n",
      "Epoch 28/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8296 - loss: 0.3753 - val_accuracy: 0.8156 - val_loss: 0.3847\n",
      "Neural Network Test Accuracy: 0.8174999952316284\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.90      0.99      0.94        88\n",
      "         Low       0.75      1.00      0.85       182\n",
      "      Medium       0.98      0.45      0.61       130\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.88      0.81      0.80       400\n",
      "weighted avg       0.86      0.82      0.80       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 87   0   1]\n",
      " [  0 182   0]\n",
      " [ 10  62  58]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\tf_env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\trejan\\tf_env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 546, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1022, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1491, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Obesity Risk Category: Low\n",
      "\n",
      "Recommended Meals:\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "1636                                     Oil, poppyseed        884.0   \n",
      "1291                                 Oil, nutmeg butter        884.0   \n",
      "523                                         Oil, canola        884.0   \n",
      "1081                                      Oil, hazelnut        884.0   \n",
      "536   Shortening, special purpose for baking, soybea...        884.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "1636        0.0  100.0     0.0  \n",
      "1291        0.0  100.0     0.0  \n",
      "523         0.0  100.0     0.0  \n",
      "1081        0.0  100.0     0.0  \n",
      "536         0.0  100.0     0.0  \n",
      "\n",
      "Trained deep learning model saved at: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\DeepLearning_Keras_EarlyStopping_Impl3.h5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import random  # For introducing randomness in recommendations\n",
    "import os  # To set environment variables\n",
    "\n",
    "# Suppress joblib warning about physical cores\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '4'\n",
    "\n",
    "#############################\n",
    "# Pipeline 1: Deep Learning for Obesity Risk Prediction\n",
    "#############################\n",
    "\n",
    "# Load genetic dataset (assumes comma-delimited)\n",
    "genetic_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\new_genetic_profiles.csv\"\n",
    "genetic_df = pd.read_csv(genetic_file_path)\n",
    "genetic_df.columns = genetic_df.columns.str.strip()\n",
    "genetic_df.fillna(\"None\", inplace=True)\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (Low, Medium, High)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.5, 0.8, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Initialize dictionary to store LabelEncoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical variables (Diet_Type, Physical_Activity)\n",
    "for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode gene variant columns as strings (so that \"None\" is encoded too)\n",
    "variant_columns = [\"MC4R_Variant\", \"PPARG_Variant\", \"FTO_Variant\", \"LEPR_Variant\"]\n",
    "for col in variant_columns:\n",
    "    genetic_df[col] = genetic_df[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target for the genetic model\n",
    "features = [\n",
    "    \"Age\", \"BMI\", \"Physical_Activity\", \"Diet_Type\",\n",
    "    \"MC4R_Present\", \"MC4R_Variant\",\n",
    "    \"PPARG_Present\", \"PPARG_Variant\",\n",
    "    \"FTO_Present\", \"FTO_Variant\",\n",
    "    \"LEPR_Present\", \"LEPR_Variant\"\n",
    "]\n",
    "target = \"Obesity_Risk_Category\"\n",
    "\n",
    "X_gen = genetic_df[features]\n",
    "y_gen = genetic_df[target]\n",
    "\n",
    "# Encode target labels (Low, Medium, High)\n",
    "target_le = LabelEncoder()\n",
    "y_encoded = target_le.fit_transform(y_gen)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gen, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build a deep learning model using Keras\n",
    "num_features = X_train_scaled.shape[1]\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(num_features,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 output nodes for 3 risk categories\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = nn_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Neural Network Test Accuracy:\", test_acc)\n",
    "\n",
    "y_pred_probs = nn_model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_le.classes_))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#############################\n",
    "# Pipeline 2: Meal Recommendation (Same as Before)\n",
    "#############################\n",
    "\n",
    "# Load the meal dataset (assumed to be comma-delimited)\n",
    "meal_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\train.csv\"\n",
    "meal_df = pd.read_csv(meal_file_path)\n",
    "\n",
    "# Preprocess nutritional features; these columns should exist in your meal dataset\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler_meal = StandardScaler()\n",
    "nutritional_features_scaled = scaler_meal.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using KMeans (e.g., 10 clusters)\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Define a meal recommendation function that uses the predicted obesity risk category\n",
    "def recommend_meals(user_profile, meal_df, nn_model, target_le, scaler, num_meals=5):\n",
    "    \"\"\"\n",
    "    user_profile: dict with genetic feature values (original, unencoded)\n",
    "    nn_model: trained neural network model\n",
    "    target_le: LabelEncoder for the target risk category\n",
    "    scaler: StandardScaler fitted on genetic features\n",
    "    \"\"\"\n",
    "    # Convert user_profile into a DataFrame\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    # Encode categorical features using stored encoders\n",
    "    for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col])\n",
    "    for col in variant_columns:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col].astype(str))\n",
    "    \n",
    "    # Ensure the user profile contains all required features; fill missing with 0\n",
    "    missing_cols = set(features) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0\n",
    "    user_profile_df = user_profile_df[features]\n",
    "    \n",
    "    # Scale the user profile using the same scaler as training\n",
    "    user_profile_scaled = scaler.transform(user_profile_df)\n",
    "    \n",
    "    # Predict obesity risk using the neural network model\n",
    "    pred_probs = nn_model.predict(user_profile_scaled)\n",
    "    predicted_category = np.argmax(pred_probs, axis=1)[0]\n",
    "    predicted_label = target_le.inverse_transform([predicted_category])[0]\n",
    "    print(f\"\\nPredicted Obesity Risk Category: {predicted_label}\")\n",
    "    \n",
    "    # Define cluster preferences based on predicted risk (example logic)\n",
    "    if predicted_label == 'Low':\n",
    "        preferred_clusters = [0, 1, 2, 3]  # Broaden the cluster selection\n",
    "        sort_by = random.choice(['Protein_g', 'Energy_kcal'])  # Randomize sorting\n",
    "        ascending = random.choice([True, False])  # Randomize order\n",
    "    elif predicted_label == 'Medium':\n",
    "        preferred_clusters = [4, 5, 6, 7]  # Broaden the cluster selection\n",
    "        sort_by = random.choice(['Energy_kcal', 'Fat_g'])  # Randomize sorting\n",
    "        ascending = random.choice([True, False])  # Randomize order\n",
    "    else:\n",
    "        preferred_clusters = [8, 9, 0, 1]  # Broaden the cluster selection\n",
    "        sort_by = random.choice(['Energy_kcal', 'Carb_g'])  # Randomize sorting\n",
    "        ascending = random.choice([True, False])  # Randomize order\n",
    "    \n",
    "    # Filter and sort meals from the preferred clusters\n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(preferred_clusters)]\n",
    "    recommended_meals = recommended_meals.sample(frac=1).reset_index(drop=True)  # Shuffle the meals\n",
    "    recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=ascending)\n",
    "    \n",
    "    print(\"\\nRecommended Meals:\")\n",
    "    print(recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']].head(num_meals))\n",
    "\n",
    "# Example new genetic profile for meal recommendation (using original, unencoded values)\n",
    "new_profile = {\n",
    "    \"Age\": 35,\n",
    "    \"BMI\": 28.5,\n",
    "    \"Physical_Activity\": \"Low\",      # Original string (will be encoded)\n",
    "    \"Diet_Type\": \"High-Fat\",           # Original string (will be encoded)\n",
    "    \"MC4R_Present\": 0,\n",
    "    \"MC4R_Variant\": \"rs17782313_TT\",\n",
    "    \"PPARG_Present\": 0,\n",
    "    \"PPARG_Variant\": \"rs1801282_CG\",\n",
    "    \"FTO_Present\": 1,\n",
    "    \"FTO_Variant\": \"rs9939609_AT\",\n",
    "    \"LEPR_Present\": 1,\n",
    "    \"LEPR_Variant\": \"rs1137101_AG\"\n",
    "}\n",
    "\n",
    "# Get meal recommendations using the neural network model\n",
    "recommend_meals(new_profile, meal_df, nn_model, target_le, scaler, num_meals=5)\n",
    "\n",
    "#############################\n",
    "# Save the Trained Deep Learning Model\n",
    "#############################\n",
    "\n",
    "# Define the save directory and filename based on the implementation method used\n",
    "save_model_dir = r\"C:\\Users\\trejan\\Desktop\\GNN\\Saved models\"\n",
    "if not os.path.exists(save_model_dir):\n",
    "    os.makedirs(save_model_dir)\n",
    "\n",
    "# Filename includes the method name: here \"DeepLearning_Keras_EarlyStopping_Impl3.h5\"\n",
    "model_filename = os.path.join(save_model_dir, \"DeepLearning_Keras_EarlyStopping_Impl3.h5\")\n",
    "\n",
    "# Save the model using Keras' save method\n",
    "nn_model.save(model_filename)\n",
    "print(f\"\\nTrained deep learning model saved at: {model_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
