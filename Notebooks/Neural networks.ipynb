{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f418bb-c99f-4908-a3e2-023f21d400d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.88      1.00      0.94        88\n",
      "         Low       0.75      0.99      0.86       182\n",
      "      Medium       0.98      0.45      0.62       130\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.87      0.82      0.81       400\n",
      "weighted avg       0.86      0.82      0.80       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 88   0   0]\n",
      " [  0 181   1]\n",
      " [ 12  59  59]]\n",
      "Model and preprocessing artifacts saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\n",
      "ROC curves saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_roc_curves.png\n",
      "Precision-Recall curves saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_precision_recall_curves.png\n",
      "Confusion Matrix heatmap saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_confusion_matrix.png\n",
      "Loss curve saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_loss_curve.png\n",
      "Learning curve saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_learning_curve.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 556, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1038, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1550, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meal dataset saved.\n",
      "Predicted Obesity Risk Category: High\n",
      "Recommended Meals:\n",
      "                                             Descrip  Energy_kcal  Protein_g  \\\n",
      "0  Infant formula, MEAD JOHNSON, ENFAMIL, Enfagro...         65.0       2.17   \n",
      "1  Fast foods, salad, vegetables tossed, without ...         82.0       7.98   \n",
      "2        Peanuts, spanish, oil-roasted, without salt        579.0      28.01   \n",
      "3     Sweet potato, frozen, cooked, baked, with salt        100.0       1.71   \n",
      "4              Peanut butter, chunk style, with salt        589.0      24.06   \n",
      "\n",
      "   Fat_g  Carb_g  \n",
      "0   2.89    7.64  \n",
      "1   4.93    1.45  \n",
      "2  49.04   17.45  \n",
      "3   0.12   23.40  \n",
      "4  49.94   21.57  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, auc, \n",
    "                             precision_recall_curve, average_precision_score)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#############################\n",
    "# Load and Preprocess Data\n",
    "#############################\n",
    "\n",
    "genetic_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\new_genetic_profiles.csv\"\n",
    "genetic_df = pd.read_csv(genetic_file_path)\n",
    "genetic_df.columns = genetic_df.columns.str.strip()\n",
    "genetic_df.fillna(\"None\", inplace=True)\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.5, 0.8, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Initialize LabelEncoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical features\n",
    "for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode genetic variants\n",
    "variant_columns = [\"MC4R_Variant\", \"PPARG_Variant\", \"FTO_Variant\", \"LEPR_Variant\"]\n",
    "for col in variant_columns:\n",
    "    genetic_df[col] = genetic_df[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"Age\", \"BMI\", \"Physical_Activity\", \"Diet_Type\",\n",
    "    \"MC4R_Present\", \"MC4R_Variant\",\n",
    "    \"PPARG_Present\", \"PPARG_Variant\",\n",
    "    \"FTO_Present\", \"FTO_Variant\",\n",
    "    \"LEPR_Present\", \"LEPR_Variant\"\n",
    "]\n",
    "target = \"Obesity_Risk_Category\"\n",
    "\n",
    "X_gen = genetic_df[features]\n",
    "y_gen = genetic_df[target]\n",
    "\n",
    "# Encode target labels\n",
    "target_le = LabelEncoder()\n",
    "y_encoded = target_le.fit_transform(y_gen)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gen, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (important for Neural Networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#############################\n",
    "# Train Neural Network (MLP)\n",
    "#############################\n",
    "\n",
    "# Define the MLP model\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers with 100 and 50 neurons\n",
    "    activation='relu',             # Activation function\n",
    "    solver='adam',                 # Optimizer\n",
    "    max_iter=500,                  # Maximum number of iterations\n",
    "    random_state=42,\n",
    "    early_stopping=True,           # Stop training if validation score doesn't improve\n",
    "    validation_fraction=0.2        # Fraction of training data to use for validation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = mlp_model.predict(X_test_scaled)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_le.classes_))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#############################\n",
    "# Save Model and Preprocessing Artifacts as Pickle Files\n",
    "#############################\n",
    "\n",
    "# Define the save directory\n",
    "save_dir = r\"C:\\Users\\trejan\\Desktop\\GNN\\Saved models\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Save the MLP model\n",
    "with open(os.path.join(save_dir, \"mlp_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(mlp_model, f)\n",
    "\n",
    "# Save the label encoders\n",
    "with open(os.path.join(save_dir, \"label_encoders.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# Save the target encoder\n",
    "with open(os.path.join(save_dir, \"target_encoder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(target_le, f)\n",
    "\n",
    "# Save the scaler\n",
    "with open(os.path.join(save_dir, \"scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"Model and preprocessing artifacts saved to: {save_dir}\")\n",
    "\n",
    "#############################\n",
    "# Additional Evaluation Metrics and Graphs for MLP Model\n",
    "#############################\n",
    "\n",
    "# Binarize the test labels for ROC and Precision-Recall curves\n",
    "n_classes = len(target_le.classes_)\n",
    "y_test_bin = label_binarize(y_test, classes=list(range(n_classes)))\n",
    "y_score = mlp_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# 1. Plot ROC Curves for each class\n",
    "plt.figure()\n",
    "colors = ['blue', 'red', 'green']  # Adjust colors if more classes are present\n",
    "for i in range(n_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
    "             label=f\"ROC curve for {target_le.classes_[i]} (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for MLP Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "roc_path = os.path.join(save_dir, 'mlp_roc_curves.png')\n",
    "plt.savefig(roc_path)\n",
    "plt.close()\n",
    "print(f\"ROC curves saved to: {roc_path}\")\n",
    "\n",
    "# 2. Plot Precision-Recall Curves for each class\n",
    "plt.figure()\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    avg_precision = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall, precision, lw=2,\n",
    "             label=f\"PR curve for {target_le.classes_[i]} (AP = {avg_precision:.2f})\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves for MLP Classifier')\n",
    "plt.legend(loc='upper right')\n",
    "pr_path = os.path.join(save_dir, 'mlp_precision_recall_curves.png')\n",
    "plt.savefig(pr_path)\n",
    "plt.close()\n",
    "print(f\"Precision-Recall curves saved to: {pr_path}\")\n",
    "\n",
    "# 3. Plot Confusion Matrix Heatmap\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
    "            xticklabels=target_le.classes_, yticklabels=target_le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for MLP Classifier')\n",
    "cm_path = os.path.join(save_dir, 'mlp_confusion_matrix.png')\n",
    "plt.savefig(cm_path)\n",
    "plt.close()\n",
    "print(f\"Confusion Matrix heatmap saved to: {cm_path}\")\n",
    "\n",
    "# 4. Plot Loss Curve (if available)\n",
    "if hasattr(mlp_model, 'loss_curve_'):\n",
    "    plt.figure()\n",
    "    plt.plot(mlp_model.loss_curve_, marker='o')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('MLP Loss Curve')\n",
    "    loss_curve_path = os.path.join(save_dir, 'mlp_loss_curve.png')\n",
    "    plt.savefig(loss_curve_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss curve saved to: {loss_curve_path}\")\n",
    "\n",
    "# 5. Plot Learning Curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    mlp_model, X_train_scaled, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')\n",
    "plt.plot(train_sizes, np.mean(valid_scores, axis=1), 'o-', label='Cross-validation score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curve for MLP Classifier')\n",
    "plt.legend(loc='best')\n",
    "learning_curve_path = os.path.join(save_dir, 'mlp_learning_curve.png')\n",
    "plt.savefig(learning_curve_path)\n",
    "plt.close()\n",
    "print(f\"Learning curve saved to: {learning_curve_path}\")\n",
    "\n",
    "#############################\n",
    "# Pipeline 2: Meal Recommendation\n",
    "#############################\n",
    "\n",
    "meal_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\train.csv\"\n",
    "meal_df = pd.read_csv(meal_file_path)\n",
    "\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features)\n",
    "\n",
    "# Save the meal dataset\n",
    "with open(os.path.join(save_dir, \"meals.pkl\"), \"wb\") as meal_file:\n",
    "    pickle.dump(meal_df, meal_file)\n",
    "\n",
    "print(\"Meal dataset saved.\")\n",
    "\n",
    "# Define a meal recommendation function\n",
    "def recommend_meals(user_profile, meal_df, mlp_model, target_le, scaler, num_meals=5):\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col])\n",
    "    for col in variant_columns:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col].astype(str))\n",
    "    \n",
    "    missing_cols = set(features) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0\n",
    "    user_profile_df = user_profile_df[features]\n",
    "\n",
    "    # Scale the user profile\n",
    "    user_profile_scaled = scaler.transform(user_profile_df)\n",
    "\n",
    "    predicted_category = mlp_model.predict(user_profile_scaled)[0]\n",
    "    predicted_label = target_le.inverse_transform([predicted_category])[0]\n",
    "    \n",
    "    if predicted_label == 'Low':\n",
    "        preferred_clusters = [0, 1, 2, 3]\n",
    "    elif predicted_label == 'Medium':\n",
    "        preferred_clusters = [4, 5, 6, 7]\n",
    "    else:\n",
    "        preferred_clusters = [8, 9]\n",
    "    \n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(preferred_clusters)]\n",
    "    recommended_meals = recommended_meals.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return predicted_label, recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']].head(num_meals)\n",
    "\n",
    "# Example user profile\n",
    "new_profile = {\n",
    "    \"Age\": 35,\n",
    "    \"BMI\": 28.5,\n",
    "    \"Physical_Activity\": \"Low\",\n",
    "    \"Diet_Type\": \"High-Fat\",\n",
    "    \"MC4R_Present\": 1,\n",
    "    \"MC4R_Variant\": \"rs17782313_TT\",\n",
    "    \"PPARG_Present\": 0,\n",
    "    \"PPARG_Variant\": \"rs1801282_CG\",\n",
    "    \"FTO_Present\": 1,\n",
    "    \"FTO_Variant\": \"rs9939609_AT\",\n",
    "    \"LEPR_Present\": 1,\n",
    "    \"LEPR_Variant\": \"rs1137101_AG\"\n",
    "}\n",
    "\n",
    "predicted_risk, recommended_meals = recommend_meals(new_profile, meal_df, mlp_model, target_le, scaler, num_meals=5)\n",
    "print(f\"Predicted Obesity Risk Category: {predicted_risk}\")\n",
    "print(\"Recommended Meals:\")\n",
    "print(recommended_meals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bed69d3-8307-4252-a2af-d46507e932ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation Importance saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_permutation_importance.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance\n",
    "result = permutation_importance(mlp_model, X_test_scaled, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Visualize\n",
    "sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(np.array(features)[sorted_idx], result.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.title(\"Permutation Feature Importance for MLP Classifier\")\n",
    "perm_path = os.path.join(save_dir, 'mlp_permutation_importance.png')\n",
    "plt.tight_layout()\n",
    "plt.savefig(perm_path)\n",
    "plt.close()\n",
    "print(f\"Permutation Importance saved to: {perm_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c05df90-78e9-4715-a1bb-515474490d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDP saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_partial_dependence_class_0.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=features)\n",
    "\n",
    "# Select most important features from permutation\n",
    "important_features = [features[i] for i in sorted_idx[:5]]  # Top 5 features\n",
    "\n",
    "# Specify class index (e.g., 0 = first class)\n",
    "target_class_index = 0  # You can change this to 1 or 2 if needed\n",
    "\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    mlp_model,\n",
    "    X_train_df,\n",
    "    features=important_features,\n",
    "    feature_names=features,\n",
    "    kind='average',\n",
    "    target=target_class_index\n",
    ")\n",
    "\n",
    "display.figure_.suptitle(f\"Partial Dependence Plots for Class {mlp_model.classes_[target_class_index]}\")\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "pdp_path = os.path.join(save_dir, f'mlp_partial_dependence_class_{target_class_index}.png')\n",
    "plt.savefig(pdp_path)\n",
    "plt.close()\n",
    "print(f\"PDP saved to: {pdp_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfc08cf-bf51-453d-865e-91d802494979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept -0.03351194969935145\n",
      "Prediction_local [0.32155442]\n",
      "Right: 0.602373424093634\n",
      "LIME explanation for sample 0 saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\lime_explanation_sample_0.html\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Prepare LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_scaled,\n",
    "    feature_names=features,\n",
    "    class_names=target_le.classes_,\n",
    "    discretize_continuous=True,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Explain a prediction (e.g., first test sample)\n",
    "i = 0\n",
    "exp = explainer.explain_instance(X_test_scaled[i], mlp_model.predict_proba, num_features=10)\n",
    "lime_path = os.path.join(save_dir, f\"lime_explanation_sample_{i}.html\")\n",
    "exp.save_to_file(lime_path)\n",
    "print(f\"LIME explanation for sample {i} saved to: {lime_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa0cc9c-ec3f-4097-8e4f-584d9ce99f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDPs for Class 0 saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_partial_dependence_plots_class_0.png\n",
      "PDPs for Class 1 saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_partial_dependence_plots_class_1.png\n",
      "PDPs for Class 2 saved to: C:\\Users\\trejan\\Desktop\\GNN\\Saved models\\mlp_partial_dependence_plots_class_2.png\n"
     ]
    }
   ],
   "source": [
    "# Loop through all classes in the model (assuming classes are labeled as integers: 0, 1, 2, etc.)\n",
    "for target_class in range(mlp_model.classes_.shape[0]):  # Loop over all classes\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        mlp_model,\n",
    "        X_train_df,\n",
    "        features=important_features,\n",
    "        feature_names=features,\n",
    "        kind=\"average\",\n",
    "        target=target_class  # Specify the target class index\n",
    "    )\n",
    "\n",
    "    # Adjust spacing between subplots for better readability\n",
    "    display.figure_.suptitle(f\"Partial Dependence Plots for MLP Classifier (Class {target_class})\")\n",
    "    plt.subplots_adjust(hspace=0.6)  # Increase space between subplots\n",
    "    pdp_path = os.path.join(save_dir, f'mlp_partial_dependence_plots_class_{target_class}.png')\n",
    "    plt.savefig(pdp_path)\n",
    "    plt.close()\n",
    "    print(f\"PDPs for Class {target_class} saved to: {pdp_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
