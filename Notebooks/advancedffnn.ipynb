{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f590bc-b878-4118-a0a3-e9a223a4b970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\tf_env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.3868 - loss: 1.0933 - val_accuracy: 0.7156 - val_loss: 0.7680\n",
      "Epoch 2/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6232 - loss: 0.7964 - val_accuracy: 0.7875 - val_loss: 0.5662\n",
      "Epoch 3/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6865 - loss: 0.6546 - val_accuracy: 0.7906 - val_loss: 0.5032\n",
      "Epoch 4/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7359 - loss: 0.5941 - val_accuracy: 0.8125 - val_loss: 0.4695\n",
      "Epoch 5/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7761 - loss: 0.5326 - val_accuracy: 0.8031 - val_loss: 0.4497\n",
      "Epoch 6/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7642 - loss: 0.5269 - val_accuracy: 0.8156 - val_loss: 0.4380\n",
      "Epoch 7/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7681 - loss: 0.5129 - val_accuracy: 0.8156 - val_loss: 0.4308\n",
      "Epoch 8/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8004 - loss: 0.4755 - val_accuracy: 0.8156 - val_loss: 0.4231\n",
      "Epoch 9/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7665 - loss: 0.4824 - val_accuracy: 0.8156 - val_loss: 0.4136\n",
      "Epoch 10/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8171 - loss: 0.4453 - val_accuracy: 0.8125 - val_loss: 0.4027\n",
      "Epoch 11/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8005 - loss: 0.4694 - val_accuracy: 0.8156 - val_loss: 0.4084\n",
      "Epoch 12/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7968 - loss: 0.4590 - val_accuracy: 0.8156 - val_loss: 0.3969\n",
      "Epoch 13/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8275 - loss: 0.4190 - val_accuracy: 0.8188 - val_loss: 0.3958\n",
      "Epoch 14/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8231 - loss: 0.4188 - val_accuracy: 0.8156 - val_loss: 0.3957\n",
      "Epoch 15/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8031 - loss: 0.4421 - val_accuracy: 0.8188 - val_loss: 0.3955\n",
      "Epoch 16/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8034 - loss: 0.4615 - val_accuracy: 0.8188 - val_loss: 0.3907\n",
      "Epoch 17/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8164 - loss: 0.4197 - val_accuracy: 0.8156 - val_loss: 0.3963\n",
      "Epoch 18/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8083 - loss: 0.4286 - val_accuracy: 0.8188 - val_loss: 0.3877\n",
      "Epoch 19/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8262 - loss: 0.4151 - val_accuracy: 0.8156 - val_loss: 0.3980\n",
      "Epoch 20/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8440 - loss: 0.3851 - val_accuracy: 0.8156 - val_loss: 0.3942\n",
      "Epoch 21/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8174 - loss: 0.4229 - val_accuracy: 0.8156 - val_loss: 0.3894\n",
      "Epoch 22/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8271 - loss: 0.4143 - val_accuracy: 0.8156 - val_loss: 0.3915\n",
      "Epoch 23/100\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8273 - loss: 0.3908 - val_accuracy: 0.8156 - val_loss: 0.3924\n",
      "Neural Network Test Accuracy: 0.8125\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.89      0.98      0.93        88\n",
      "         Low       0.75      1.00      0.85       182\n",
      "      Medium       0.97      0.44      0.60       130\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.87      0.81      0.80       400\n",
      "weighted avg       0.85      0.81      0.79       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 86   0   2]\n",
      " [  0 182   0]\n",
      " [ 11  62  57]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trejan\\tf_env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\trejan\\tf_env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 546, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1022, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\trejan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1491, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\n",
      "Predicted Obesity Risk Category: Medium\n",
      "\n",
      "Recommended Meals:\n",
      "                                                Descrip  Energy_kcal  \\\n",
      "455      Egg, whole, dried, stabilized, glucose reduced        615.0   \n",
      "3528  Snacks, popcorn, oil-popped, microwave, regula...        583.0   \n",
      "3506                                  Egg, whole, dried        605.0   \n",
      "3149          Puff pastry, frozen, ready-to-bake, baked        558.0   \n",
      "3337          Candies, HERSHEY'S POT OF GOLD Almond Bar        577.0   \n",
      "\n",
      "      Protein_g  Fat_g  Carb_g  \n",
      "455       48.17  43.95    2.38  \n",
      "3528       7.29  43.55   45.06  \n",
      "3506      48.37  43.04    1.53  \n",
      "3149       7.40  38.50   45.70  \n",
      "3337      12.82  38.46   46.15  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Low'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14492\\3509371066.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;34m'Activity Level'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Low'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;34m'Diet Type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'High-Fat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;34m'MC4R Gene'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Present'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;34m'LEPR Gene'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Present'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m     \u001b[1;34m'Predicted Risk'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_profile\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m })\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'green'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'orange'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'red'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\tf_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             return_tuple = (\n",
      "\u001b[1;32m~\\tf_env\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \"\"\"\n\u001b[0;32m   1059\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m         X = validate_data(\n\u001b[0m\u001b[0;32m   1063\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m             \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\tf_env\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2940\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2941\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2944\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2945\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2947\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\tf_env\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1052\u001b[0m                         )\n\u001b[0;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m                 raise ValueError(\n\u001b[0;32m   1058\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32m~\\tf_env\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\tf_env\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Low'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import random  # For introducing randomness in recommendations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Suppress joblib warning about physical cores\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '4'\n",
    "\n",
    "# Define visualization save directory \n",
    "visualization_dir = r\"C:\\Users\\trejan\\Desktop\\GNN\"\n",
    "if not os.path.exists(visualization_dir):\n",
    "    os.makedirs(visualization_dir)\n",
    "\n",
    "#############################\n",
    "# Pipeline 1: Deep Learning for Obesity Risk Prediction\n",
    "#############################\n",
    "\n",
    "# Load genetic dataset (assumes comma-delimited)\n",
    "genetic_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\new_genetic_profiles.csv\"\n",
    "genetic_df = pd.read_csv(genetic_file_path)\n",
    "genetic_df.columns = genetic_df.columns.str.strip()\n",
    "genetic_df.fillna(\"None\", inplace=True)\n",
    "\n",
    "# Convert Obesity_Risk_Score into categories (Low, Medium, High)\n",
    "genetic_df['Obesity_Risk_Category'] = pd.cut(\n",
    "    genetic_df['Obesity_Risk_Score'],\n",
    "    bins=[0, 0.5, 0.8, 1],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Initialize dictionary to store LabelEncoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical variables (Diet_Type, Physical_Activity)\n",
    "for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode gene variant columns as strings (so that \"None\" is encoded too)\n",
    "variant_columns = [\"MC4R_Variant\", \"PPARG_Variant\", \"FTO_Variant\", \"LEPR_Variant\"]\n",
    "for col in variant_columns:\n",
    "    genetic_df[col] = genetic_df[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    genetic_df[col] = le.fit_transform(genetic_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target for the genetic model\n",
    "features = [\n",
    "    \"Age\", \"BMI\", \"Physical_Activity\", \"Diet_Type\",\n",
    "    \"MC4R_Present\", \"MC4R_Variant\",\n",
    "    \"PPARG_Present\", \"PPARG_Variant\",\n",
    "    \"FTO_Present\", \"FTO_Variant\",\n",
    "    \"LEPR_Present\", \"LEPR_Variant\"\n",
    "]\n",
    "target = \"Obesity_Risk_Category\"\n",
    "\n",
    "X_gen = genetic_df[features]\n",
    "y_gen = genetic_df[target]\n",
    "\n",
    "# Create a visual for feature distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate([\"Age\", \"BMI\", \"Physical_Activity\", \"Diet_Type\"]):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    if feature in [\"Physical_Activity\", \"Diet_Type\"]:\n",
    "        counts = genetic_df[feature].value_counts()\n",
    "        sns.barplot(x=counts.index, y=counts.values)\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        if feature == \"Physical_Activity\":\n",
    "            plt.xlabel(f'{feature} (0=Low, 1=Moderate, 2=High)')\n",
    "        elif feature == \"Diet_Type\":\n",
    "            plt.xlabel(f'{feature} (Encoded)')\n",
    "    else:\n",
    "        sns.histplot(genetic_df[feature], kde=True)\n",
    "        plt.title(f'Distribution of {feature}')\n",
    "        plt.xlabel(feature)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_dir, \"feature_distributions.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Visual for risk category distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "risk_counts = genetic_df['Obesity_Risk_Category'].value_counts()\n",
    "sns.barplot(x=risk_counts.index, y=risk_counts.values)\n",
    "plt.title('Distribution of Obesity Risk Categories')\n",
    "plt.xlabel('Risk Category')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(os.path.join(visualization_dir, \"risk_category_distribution.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Encode target labels (Low, Medium, High)\n",
    "target_le = LabelEncoder()\n",
    "y_encoded = target_le.fit_transform(y_gen)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gen, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Visualize the scaled features using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Create a scatter plot of the PCA components colored by risk category\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['green', 'orange', 'red']\n",
    "for i, risk_level in enumerate(['Low', 'Medium', 'High']):\n",
    "    indices = np.where(y_train == i)\n",
    "    plt.scatter(X_pca[indices, 0], X_pca[indices, 1], c=colors[i], label=risk_level, alpha=0.7)\n",
    "plt.title('PCA of Genetic Features by Obesity Risk Category')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig(os.path.join(visualization_dir, \"pca_visualization.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Build a deep learning model using Keras\n",
    "num_features = X_train_scaled.shape[1]\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(num_features,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 output nodes for 3 risk categories\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Visualize the model training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_dir, \"model_training_history.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = nn_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Neural Network Test Accuracy:\", test_acc)\n",
    "\n",
    "y_pred_probs = nn_model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_le.classes_))\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_le.classes_,\n",
    "            yticklabels=target_le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create feature importance visualization using a simpler model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = pd.Series(rf_model.feature_importances_, index=features)\n",
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "sns.barplot(x=feature_importance.values, y=feature_importance.index)\n",
    "plt.title('Feature Importance for Obesity Risk Prediction')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_dir, \"feature_importance.png\"))\n",
    "plt.close()\n",
    "\n",
    "#############################\n",
    "# Pipeline 2: Meal Recommendation\n",
    "#############################\n",
    "\n",
    "# Load the meal dataset (assumed to be comma-delimited)\n",
    "meal_file_path = r\"C:\\Users\\trejan\\Desktop\\Sem 2\\Machine Learning\\model\\train.csv\"\n",
    "meal_df = pd.read_csv(meal_file_path)\n",
    "\n",
    "# Visualize distributions of nutritional values\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(meal_df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_dir, \"nutrition_distributions.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Preprocess nutritional features; these columns should exist in your meal dataset\n",
    "nutritional_features = meal_df[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "scaler_meal = StandardScaler()\n",
    "nutritional_features_scaled = scaler_meal.fit_transform(nutritional_features)\n",
    "\n",
    "# Cluster meals using KMeans (e.g., 10 clusters)\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "meal_df['Meal_Cluster'] = kmeans.fit_predict(nutritional_features_scaled)\n",
    "\n",
    "# Visualize the clusters using PCA\n",
    "pca_meal = PCA(n_components=2)\n",
    "meal_pca = pca_meal.fit_transform(nutritional_features_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, num_clusters))\n",
    "for i in range(num_clusters):\n",
    "    plt.scatter(meal_pca[meal_df['Meal_Cluster'] == i, 0],\n",
    "                meal_pca[meal_df['Meal_Cluster'] == i, 1],\n",
    "                c=[colors[i]], label=f'Cluster {i}', alpha=0.7)\n",
    "plt.title('PCA of Meal Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.savefig(os.path.join(visualization_dir, \"meal_clusters_pca.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create radar chart to visualize nutrition profile of each cluster\n",
    "# Calculate mean nutrition values for each cluster\n",
    "cluster_means = meal_df.groupby('Meal_Cluster')[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']].mean()\n",
    "\n",
    "# Normalize the cluster means for radar chart\n",
    "cluster_means_normalized = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
    "\n",
    "# Create radar charts for each cluster\n",
    "categories = ['Energy', 'Protein', 'Fat', 'Carbs']\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Maximum clusters to show in one figure (e.g., 5 clusters)\n",
    "clusters_per_fig = 5\n",
    "for fig_num in range(2):  # Create 2 figures for all 10 clusters\n",
    "    start_cluster = fig_num * clusters_per_fig\n",
    "    end_cluster = min(start_cluster + clusters_per_fig, num_clusters)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    for i in range(start_cluster, end_cluster):\n",
    "        # Number of variables\n",
    "        N = len(categories)\n",
    "        \n",
    "        # What will be the angle of each axis in the plot\n",
    "        angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        # Create subplot\n",
    "        ax = plt.subplot(2, 3, i - start_cluster + 1, polar=True)\n",
    "        \n",
    "        # Draw one axis per variable and add labels\n",
    "        plt.xticks(angles[:-1], categories, size=10)\n",
    "        \n",
    "        # Draw the values for this cluster\n",
    "        values = cluster_means_normalized.iloc[i].values.flatten().tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'Cluster {i}')\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "        # Add title\n",
    "        plt.title(f'Cluster {i} Nutrition Profile', size=11, y=1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(visualization_dir, f\"cluster_nutrition_radar_{fig_num+1}.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "# Define a meal recommendation function that uses the predicted obesity risk category\n",
    "def recommend_meals(user_profile, meal_df, nn_model, target_le, scaler, visualization_dir, num_meals=5):\n",
    "    \"\"\"\n",
    "    user_profile: dict with genetic feature values (original, unencoded)\n",
    "    nn_model: trained neural network model\n",
    "    target_le: LabelEncoder for the target risk category\n",
    "    scaler: StandardScaler fitted on genetic features\n",
    "    \"\"\"\n",
    "    # Convert user_profile into a DataFrame\n",
    "    user_profile_df = pd.DataFrame([user_profile])\n",
    "    \n",
    "    # Encode categorical features using stored encoders\n",
    "    for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col])\n",
    "    for col in variant_columns:\n",
    "        if col in user_profile_df.columns:\n",
    "            le = label_encoders[col]\n",
    "            user_profile_df[col] = le.transform(user_profile_df[col].astype(str))\n",
    "    \n",
    "    # Ensure the user profile contains all required features; fill missing with 0\n",
    "    missing_cols = set(features) - set(user_profile_df.columns)\n",
    "    for col in missing_cols:\n",
    "        user_profile_df[col] = 0\n",
    "    user_profile_df = user_profile_df[features]\n",
    "    \n",
    "    # Scale the user profile using the same scaler as training\n",
    "    user_profile_scaled = scaler.transform(user_profile_df)\n",
    "    \n",
    "    # Predict obesity risk using the neural network model\n",
    "    pred_probs = nn_model.predict(user_profile_scaled)\n",
    "    predicted_category = np.argmax(pred_probs, axis=1)[0]\n",
    "    predicted_label = target_le.inverse_transform([predicted_category])[0]\n",
    "    print(f\"\\nPredicted Obesity Risk Category: {predicted_label}\")\n",
    "    \n",
    "    # Define cluster preferences based on predicted risk (example logic)\n",
    "    if predicted_label == 'Low':\n",
    "        preferred_clusters = [0, 1, 2, 3]  # Broaden the cluster selection\n",
    "        sort_by = random.choice(['Protein_g', 'Energy_kcal'])  # Randomize sorting\n",
    "        ascending = random.choice([True, False])  # Randomize order\n",
    "    elif predicted_label == 'Medium':\n",
    "        preferred_clusters = [4, 5, 6, 7]  # Broaden the cluster selection\n",
    "        sort_by = random.choice(['Energy_kcal', 'Fat_g'])  # Randomize sorting\n",
    "        ascending = random.choice([True, False])  # Randomize order\n",
    "    else:\n",
    "        preferred_clusters = [8, 9, 0, 1]  # Broaden the cluster selection\n",
    "        sort_by = random.choice(['Energy_kcal', 'Carb_g'])  # Randomize sorting\n",
    "        ascending = random.choice([True, False])  # Randomize order\n",
    "    \n",
    "    # Filter and sort meals from the preferred clusters\n",
    "    recommended_meals = meal_df[meal_df['Meal_Cluster'].isin(preferred_clusters)]\n",
    "    recommended_meals = recommended_meals.sample(frac=1).reset_index(drop=True)  # Shuffle the meals\n",
    "    recommended_meals = recommended_meals.sort_values(by=sort_by, ascending=ascending)\n",
    "    \n",
    "    print(\"\\nRecommended Meals:\")\n",
    "    top_meals = recommended_meals[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']].head(num_meals)\n",
    "    print(top_meals)\n",
    "    \n",
    "    # Create a visualization of the recommended meals\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    top_meals_for_viz = top_meals.reset_index(drop=True)\n",
    "    \n",
    "    # Create a subplot for each nutritional component\n",
    "    for i, nutrient in enumerate(['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        bars = plt.bar(top_meals_for_viz.index, top_meals_for_viz[nutrient], color='skyblue')\n",
    "        plt.title(f'{nutrient} in Recommended Meals')\n",
    "        plt.xticks(top_meals_for_viz.index, [f\"Meal {i+1}\" for i in range(len(top_meals_for_viz))], rotation=45)\n",
    "        plt.ylabel(nutrient)\n",
    "        \n",
    "        # Add the meal names as text above each bar\n",
    "        for idx, bar in enumerate(bars):\n",
    "            meal_name = top_meals_for_viz['Descrip'].iloc[idx]\n",
    "            if len(meal_name) > 15:\n",
    "                meal_name = meal_name[:15] + \"...\"\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    meal_name, ha='center', va='bottom', rotation=45, fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(visualization_dir, \"recommended_meals.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a parallel coordinates plot to compare nutritional values\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Normalize data for better visualization\n",
    "    meal_data = top_meals[['Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g']]\n",
    "    meal_data_norm = (meal_data - meal_data.min()) / (meal_data.max() - meal_data.min())\n",
    "    meal_data_norm['Meal'] = [f\"Meal {i+1}\" for i in range(len(meal_data_norm))]\n",
    "    \n",
    "    # Create parallel coordinates\n",
    "    pd.plotting.parallel_coordinates(meal_data_norm, 'Meal', colormap=plt.cm.tab10)\n",
    "    plt.title('Nutritional Comparison of Recommended Meals')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.savefig(os.path.join(visualization_dir, \"meal_nutrition_comparison.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return top_meals\n",
    "\n",
    "# Example new genetic profile for meal recommendation (using original, unencoded values)\n",
    "new_profile = {\n",
    "    \"Age\": 35,\n",
    "    \"BMI\": 28.5,\n",
    "    \"Physical_Activity\": \"Low\",      # Original string (will be encoded)\n",
    "    \"Diet_Type\": \"High-Fat\",           # Original string (will be encoded)\n",
    "    \"MC4R_Present\": 1,\n",
    "    \"MC4R_Variant\": \"rs17782313_TT\",\n",
    "    \"PPARG_Present\": 0,\n",
    "    \"PPARG_Variant\": \"rs1801282_CG\",\n",
    "    \"FTO_Present\": 0,\n",
    "    \"FTO_Variant\": \"rs9939609_AT\",\n",
    "    \"LEPR_Present\": 1,\n",
    "    \"LEPR_Variant\": \"rs1137101_AG\"\n",
    "}\n",
    "\n",
    "# Get meal recommendations using the neural network model\n",
    "recommended_meals = recommend_meals(new_profile, meal_df, nn_model, target_le, scaler, visualization_dir, num_meals=5)\n",
    "\n",
    "# Create a visualization of the user profile\n",
    "plt.figure(figsize=(12, 8))\n",
    "user_data = pd.Series({\n",
    "    'Age': new_profile['Age'],\n",
    "    'BMI': new_profile['BMI'],\n",
    "    'Activity Level': 'Low',\n",
    "    'Diet Type': 'High-Fat',\n",
    "    'MC4R Gene': 'Present',\n",
    "    'LEPR Gene': 'Present',\n",
    "    'Predicted Risk': target_le.inverse_transform([np.argmax(nn_model.predict(scaler.transform(pd.DataFrame([new_profile])[features])), axis=1)[0]])[0]\n",
    "})\n",
    "\n",
    "colors = ['green', 'orange', 'red']\n",
    "risk_color = colors[list(target_le.classes_).index(user_data['Predicted Risk'])]\n",
    "\n",
    "# Create a horizontal bar chart for the user profile\n",
    "categorical_data = user_data.drop(['Age', 'BMI'])\n",
    "plt.subplot(2, 1, 1)\n",
    "bars = plt.barh(range(len(categorical_data)), [1] * len(categorical_data), color='lightgray')\n",
    "for i, (key, value) in enumerate(categorical_data.items()):\n",
    "    plt.text(0.5, i, f\"{key}: {value}\", ha='center', va='center', fontsize=12)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.title('User Profile Summary', fontsize=14)\n",
    "\n",
    "# Add numerical data as text\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.axis('off')\n",
    "plt.text(0.5, 0.7, f\"Age: {user_data['Age']} years\", ha='center', fontsize=14)\n",
    "plt.text(0.5, 0.5, f\"BMI: {user_data['BMI']} (Overweight)\", ha='center', fontsize=14)\n",
    "plt.text(0.5, 0.3, f\"Predicted Risk: {user_data['Predicted Risk']}\", ha='center', fontsize=16, color=risk_color, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_dir, \"user_profile_summary.png\"))\n",
    "plt.close()\n",
    "\n",
    "#############################\n",
    "# Save the Trained Deep Learning Model\n",
    "#############################\n",
    "\n",
    "# Define the save directory and filename based on the implementation method used\n",
    "save_model_dir = r\"C:\\Users\\trejan\\Desktop\\GNN\\Saved models\"\n",
    "if not os.path.exists(save_model_dir):\n",
    "    os.makedirs(save_model_dir)\n",
    "\n",
    "# Filename includes the method name: here \"DeepLearning_Keras_EarlyStopping.h5\"\n",
    "model_filename = os.path.join(save_model_dir, \"DeepLearning_Keras_EarlyStopping.h5\")\n",
    "\n",
    "# Save the model using Keras' save method\n",
    "nn_model.save(model_filename)\n",
    "print(f\"\\nTrained deep learning model saved at: {model_filename}\")\n",
    "\n",
    "# Save a model architecture visualization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "try:\n",
    "    plot_model(nn_model, to_file=os.path.join(visualization_dir, \"model_architecture.png\"), show_shapes=True, show_layer_names=True)\n",
    "    print(f\"Model architecture visualization saved to {visualization_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate model architecture visualization. Error: {e}\")\n",
    "    print(\"Note: This requires pydot and graphviz to be installed.\")\n",
    "\n",
    "print(\"\\nAll visualizations have been saved to:\", visualization_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f2c06c-f3b3-4a79-b0e5-3fbf220539dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create a visualization of the user profile - Fixed version\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# First, properly process the user profile for prediction\n",
    "user_profile_df = pd.DataFrame([new_profile])\n",
    "    \n",
    "# Encode categorical features using stored encoders\n",
    "for col in [\"Diet_Type\", \"Physical_Activity\"]:\n",
    "    if col in user_profile_df.columns:\n",
    "        le = label_encoders[col]\n",
    "        user_profile_df[col] = le.transform(user_profile_df[col])\n",
    "for col in variant_columns:\n",
    "    if col in user_profile_df.columns:\n",
    "        le = label_encoders[col]\n",
    "        user_profile_df[col] = le.transform(user_profile_df[col].astype(str))\n",
    "\n",
    "# Ensure the user profile contains all required features\n",
    "missing_cols = set(features) - set(user_profile_df.columns)\n",
    "for col in missing_cols:\n",
    "    user_profile_df[col] = 0\n",
    "user_profile_df = user_profile_df[features]\n",
    "\n",
    "# Scale the user profile and predict\n",
    "user_profile_scaled = scaler.transform(user_profile_df)\n",
    "pred_probs = nn_model.predict(user_profile_scaled)\n",
    "predicted_category = np.argmax(pred_probs, axis=1)[0]\n",
    "predicted_label = target_le.inverse_transform([predicted_category])[0]\n",
    "\n",
    "# Now create the visualization with the prediction result\n",
    "user_data = pd.Series({\n",
    "    'Age': new_profile['Age'],\n",
    "    'BMI': new_profile['BMI'],\n",
    "    'Activity Level': new_profile['Physical_Activity'],  # Use the original string\n",
    "    'Diet Type': new_profile['Diet_Type'],               # Use the original string\n",
    "    'MC4R Gene': 'Present' if new_profile['MC4R_Present'] == 1 else 'Absent',\n",
    "    'LEPR Gene': 'Present' if new_profile['LEPR_Present'] == 1 else 'Absent',\n",
    "    'Predicted Risk': predicted_label\n",
    "})\n",
    "\n",
    "colors = ['green', 'orange', 'red']\n",
    "risk_color = colors[list(target_le.classes_).index(user_data['Predicted Risk'])]\n",
    "\n",
    "# Create a horizontal bar chart for the user profile\n",
    "categorical_data = user_data.drop(['Age', 'BMI'])\n",
    "plt.subplot(2, 1, 1)\n",
    "bars = plt.barh(range(len(categorical_data)), [1] * len(categorical_data), color='lightgray')\n",
    "for i, (key, value) in enumerate(categorical_data.items()):\n",
    "    plt.text(0.5, i, f\"{key}: {value}\", ha='center', va='center', fontsize=12)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.title('User Profile Summary', fontsize=14)\n",
    "\n",
    "# Add numerical data as text\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.axis('off')\n",
    "plt.text(0.5, 0.7, f\"Age: {user_data['Age']} years\", ha='center', fontsize=14)\n",
    "plt.text(0.5, 0.5, f\"BMI: {user_data['BMI']} (Overweight)\", ha='center', fontsize=14)\n",
    "plt.text(0.5, 0.3, f\"Predicted Risk: {user_data['Predicted Risk']}\", ha='center', fontsize=16, color=risk_color, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualization_dir, \"user_profile_summary.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
